\section{Methods}
\subsection{Background}

\subsubsection{Problem formulation}\ 

\noindent The protein design problem can be described as a pairwise markov networks. All the residues of the protein can be regarded as the vertices of the network, and the interaction between residues is the edges. Let $G=(V,E)$ be the markov network of the protein design problem, then for each vertex $v\in V$, there is an available states set $X_v$, which is the rotamer set of the residue position $v$ in the protein chain. The aim of the CPD problem is to find the optimal assignment $\mathbf{x^*}$ that minimizes the total energy, namely
\[ \mathbf{x^*} = \arg\min_{\mathbf{x}}\left(\sum_{v\in V}\theta_v(x_v) + \sum_{(u,v)\in E}\theta_{uv}(x_u,x_v)\right) \]
where $x_v\in X_v$ is the rotamer at residue position $v$, $\theta_v(x_v)$ is the potential energy between the internal atoms of the rotamer, and $\theta_{uv}(x_u,x_v)$ is the potential energy between rotamer $x_u$ at residue position $u$ and $x_v$.

In order to find the optimal solution of a NP-hard problem, we often need to search over a huge state space. As many popular methods to solve the CPD problem, firstly use dead-end elimination as a pre-filtering algorithm to prune the rotamers that are not part of the GMEC, and thus reduce the search space. Then we use branch-and-bound algorithm on a distributed computing platform to search the remain search space.

\subsubsection{Branch-and-Bound}\ 

\noindent Branch-and-Bound (BnB) is a widely-used search algorithm for solving various combinatorial optimization problems. This algorithm constantly divides the state space into several smaller sub-spaces (this step is called \textit{branching}) and then calculate the bound for each sub-spaces (this step is called \textit{bounding}). After that, those sub-spaces which certainly not contain the optimal solution (i.e. the lower bound is larger than the known upper bound) are discarded.

To be more specific, suppose we want to solve an optimization problem through minimizing an energy function over a state space $S$. The algorithm has two main steps:

\noindent \textbf{Branching}: In this step, the state space $S$ is split into two or more sub-space $S_1,S_2,\dots,S_m$ such that $S_1\cup S_2\cup\dots\cup S_m=S$ and $S_i\cap S_j=\emptyset$ for all $i\neq j$.

\noindent \textbf{Bounding}: In this step, we compute the lower bound and upper bound of each sub-space $S_i$, denote by $LB(S_i)$ and $UB(S_i)$. Let $GUB=\min_{1\le i\le m}UB(S_i)$ be the minimum upper bound, then we can prune the sub-space $S_i$ if $LB(S_i)>GUB$, since there exists an element that is better than all elements of $S_i$.

The aforementioned combination of \textit{branching} and \textit{bounding} steps is recursively performed until the state space only contains a single element.

Assume we the network contains $n$ vertices which are numbered from 1 to $n$, and let $X_i$ denote the state set of vertex $i$. Here is the pseudo code of Branch-and-Bound algorithm.

\begin{algorithm}[!h]
\caption{Branch-and-Bound Algorithm}
\begin{algorithmic}[1]
\Function{BranchAndBound}{$G=(V,E),X,\Theta$}
    \State $S_V \gets X_1\times X_2\times\dots\times X_n$
    \State $GUB \gets \textsc{UpperBound}(S_V)$
    \State $\textsc{Add}(Q, S_V)$
    \While{$Q$ is not empty}
        \State $S \gets \textsc{NextElement}(Q)$
        \If {$\textsc{LowerBound}(S)\geq GUB$}
            \State \textbf{continue}
        \EndIf
        \State $(S_1,S_2,...,S_m) \gets \textsc{Branch}(S)$
        \For {$i \gets 1$\textbf{ to }$m$}
            \State $GUB \gets \min(GUB, \textsc{UpperBound}(S_i))$
        \EndFor
        \For {$i \gets 1$\textbf{ to }$m$}
            \If {$\textsc{LowerBound}(S_i) < GUB$}
                \State $\textsc{Add}(Q, S_i)$
            \EndIf
        \EndFor
    \EndWhile
    \State \textbf{return }$GUB$
\EndFunction
\end{algorithmic}
\end{algorithm}

Here $Q$ is usually a FIFO or priority queue, and we maintain a global variable $GUB$ to store the best solution. An efficient lower bound of a state space will be proposed in Section 2.2.2, and upper bound in Section 2.2.3.

\subsubsection{Local Dead-End Elimination Algorithm}\ 

\noindent The dead-end elimination (DEE) algorithm is an efficient method to eliminate infeasible variable states. For a variable $x_v$, and two variable states $x_v^i$ and $x_v^j$ in $X_v$, if the following condition is satisfied, then state $x_v^i$ can be eliminated, which reduces the search space.
\begin{align}
& \theta_v(x_v^i)+\sum_{(u,v)\in E}\min_{x_u\in X_u}\theta_{uv}(x_u,x_v^i) \notag \\
>\ \ & \theta_v(x_v^j)+\sum_{(u,v)\in E}\max_{x_u\in X_u}\theta_{uv}(x_u,x_v^j)
\end{align}

The more powerful criterion that improved by \cite{goldstein1994efficient} is
\begin{align}
\theta_v(x_v^i)-\theta_v(x_v^j)+\sum_{(u,v)\in E}\min_{x_u\in X_u}[\theta_{uv}(x_u,x_v^i)-\theta_{uv}(x_u,x_v^j)] > 0
\end{align}

We apply the Goldstein DEE criterion in (2) to the function \textsc{Branch}. Let $D(X)$ be the set of variables that have been searched, and $U(X)=V\setminus D(X)$ be the set of variables which has not been determined yet. Consider two variable states $x_v^i$ and $x_v^j$ in an undetermined variable $x_v$, the Goldstein DEE criterion we use in the \textsc{Branch} functions is
\begin{align}
& \theta_v(x_v^i)-\theta_v(x_v^j)+\sum_{(u,v)\in E\wedge u\in D(X)}[\theta_{uv}(x_u,x_v^i)-\theta_{uv}(x_u,x_v^j)] \notag \\
& +\sum_{(u,v)\in E\wedge u\in U(X)}\min_{x_u\in X_u}[\theta_{uv}(x_u,x_v^i)-\theta_{uv}(x_u,x_v^j)] > 0
\end{align}

By applying the DEE criterion in Eq. (3), we can eliminate a large number of infeasible variable states, and thus significantly reduces the branch space.

Here, the DEE criterion is incorporated into each branch step. To distinguish it from most other DEE criteria that are applied before search algorithms (e.g. A*\cite[]{gainza2013osprey}), we call the criterion in Eq. (3) integrated into the branch step the \textit{local DEE criterion}.

\subsubsection{Lower Bound}\

\noindent\textbf{Naive Lower Bound.}
A naive lower bound of the energy function in protein design can be easily computed by considering the best possible rotamer assignment in each residue, which is
\begin{align}
\sum_{v\in V}\min_{x_v\in X_v}\left(\theta_v(x_v)+\sum_{u<v\wedge(u,v)\in E}\min_{x_u\in X_u}\theta_{uv}(x_u,x_v)\right)
\end{align}

That is, the naive lower bound of the current state space $X$ can be written as
\begin{align}
LB_1(X)=&g(X)+\sum_{v\in U(x)}\min_{x_v\in X_v}\left(\theta_v(x_v)+\sum_{u\in D(X)}\theta_{uv}(x_u,x_v)\notag\right. \\
&\left.+\sum_{u<v\wedge u\in U(X)}\min_{x_u\in X_u}\theta_{uv}(x_u,x_v)\right)
\end{align}
where we leave $(u,v)\in E$ out from the summation notation for simplifying the expression, and $g(X)$ is the energy of the determined variables (i.e. those residues in which the rotamers have been determined), that is
\[
g(X) = \sum_{v\in D(X)}\theta_v(x_v)+\sum_{u,v\in D(X)\wedge u<v}\theta_{uv}(x_u,x_v)
\]

\noindent\textbf{Efficient Lower Bound.} By observing the formula of the naive lower bound in Eq. (4), we see that every edge-energy function is only used for one vertex (i.e. the vertex has greater index in Eq. (4)). If we split $\theta_{uv}$ into two functions $\beta_{uv}$ and $\beta_{vu}$ where $\beta_{uv}(x_u,x_v)+\beta_{vu}(x_v,x_u)=\theta_{uv}(x_u,x_v)$ for all $x_u,x_v$, then the formula of (4) becomes
\begin{align}
\max & \ \ \sum_{v\in V}\min_{x_v\in X_v}\left(\theta_v(x_v)+\sum_{(u,v)\in E}\min_{x_u\in X_u}\beta_{uv}(x_u,x_v)\right) \\
s.t. & \ \ \beta_{uv}(x_u,x_v)+\beta_{vu}(x_v,x_u)=\theta_{vu}(x_u,x_v) \notag \\
& \ \ \forall (u,v)\in E,x_u\in X_u,x_v\in X_v \notag
\end{align}

The above optimization problem is a convex dual of MAPLPR, which can be solved by Convergent Message Passing Algorithms~\cite[]{globerson2008fixing}.

If we compute the best functions $\beta^*$, then the lower bound of the current state space $X$ becomes
\begin{align}
LB_2(X)=&g(X)+\sum_{v\in U(X)}\min_{x_v\in X_v}\left(\theta_v(x_v)+\sum_{u\in D(X)}\theta_{uv}(x_u,x_v)\right. \notag \\
&\left.+\sum_{u\in U(X)}\min_{x_u\in X_u}\beta_{uv}^*(x_u,x_v)\right)
\end{align}

Since $\beta^*$ may not be the best functions for any state space, it is necessary because we can not compute it for all the searched state space.

If we compute $LB_2$ directly, then the time complexity is $O(n^2m^2)$, where $n$ is the number of mutable residues, and $m$ is the number of rotamers per residue. If we firstly compute a table $p$ that $p_{uv}(x_v)=\min_{x_u\in X_u}\beta_{uv}^*(x_u,x_v)$ with time $O(n^2m^2)$ and space $O(n^2m)$, then the time of computing $LB_2$ decrease to $O(n^2m)$.

\noindent\textbf{Mini-Bucket.} The mini-bucket elimination (MBE) is a well-known approximation algorithm for graphical models~\cite[]{dechter2003mini, rollon2010evaluating, rollon2006mini}, and it gives a bound when the induced width of the graph is too large. The idea of mini-bucket elimination is to eliminate variables, and the time and space complexity of MBE is $O(m^i)$ where $i$ is a user controlled parameter that restrict the size of the scopes of each functions. We also apply this algorithm in our method to 

\subsection{Upper Bound}
Upper bound is different from lower bound, we often use a relatively better solution in $X$ as its upper bound. There are many meta-heuristic methods have been applied to it, such as Monte-Carlo with simulated annealing~\cite[]{kuhlman2000native, voigt2000trading}, and genetic algorithms~\cite[]{raha2000prediction}. These approaches can usually find a relatively better solution quickly but without any guarantees of accuracy. Thus, these methods provide us with an efficient upper bound.

In our method, we choose simulated annealing as our upper bound algorithm. Simulated annealing (SA) is a generic probabilistic meta-heuristic method for the global optimization problem, and it is often used when the search space is discrete. The SA heuristic is started with an arbitrary initial state. At each step, consider a neighbouring state $s'$ of the current state $s$, and probabilistically decides between moving to $s'$ or staying in $s$. The probabilities ultimately lead the system to the states with lower energy.

The initial state $\mathbf{x}^0$ of our SA heuristic is based on the lower bound function $LB_2$, which is
\begin{align*}
\mathbf{x}_v^0\!=\!\arg\!\min_{x_v\in X_v}\left(\theta_v(x_v)\!+\!\sum_{u\in D(X)}\theta_{uv}(x_u,x_v)\!+\!\sum_{u\in U(X)}p_{uv}(x_v)\right)
\end{align*}

Let $\mathbf{x}^{S}$ be the best solution found by Simulated Annealing Algorithm, then
\begin{align*}
  UB(X)=g(\mathbf{x}^{S})
\end{align*}

The time complexity of the SA heuristic $T_{SA}$ is based on the number of iteration rounds $I$ and the time of calculating the energy function $T_{EF}$, that is $T_{SA}=I*T_{EF}$. Consider the current state $\mathbf{x}$ and a neighbouring state $\mathbf{x}'$, there is only one rotamer at some residue position is different, namely $\mathbf{x}=(x_1,x_2,...,x_i,...,x_n)$ and $\mathbf{x}'=(x_1,x_2,...,x_i',...,x_n)$ at some position $i$. We have already known the energy $g(\mathbf{x})$ of $\mathbf{x}$, and now we can compute the energy of the neighboring state $\mathbf{x}'$ as follows.
\begin{align*}
g(\mathbf{x}')=g(\mathbf{x})&-\left(\theta_i(x_i)+\sum_{j\neq i}\theta_{ji}(x_j,x_i)\right)\\
&+\left(\theta_i(x_i')+\sum_{j\neq i}\theta_{ji}(x_j,x_i')\right)
\end{align*}

Thus the time $T_{EF}$ is optimized to $O(n)$ where $n$ is the number of mutable residues, and then $T_{SA}=O(n^2+I*n)$ where $n^2$ is the time of calculating the energy of the initial state.
